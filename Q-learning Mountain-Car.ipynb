{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import envs\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(2,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_status(_observation):\n",
    "    env_low = env.observation_space.low \n",
    "    env_high = env.observation_space.high \n",
    "    env_dx = (env_high - env_low) / 40  \n",
    "  \n",
    "    position = int((_observation[0] - env_low[0])/env_dx[0])\n",
    "    velocity = int((_observation[1] - env_low[1])/env_dx[1])\n",
    "    return position, velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_table(_q_table, _action,  _observation, _next_observation, _reward, _episode):\n",
    "\n",
    "    alpha = 0.2 \n",
    "    gamma = 0.99\n",
    "\n",
    "    \n",
    "    next_position, next_velocity = get_status(_next_observation)\n",
    "    next_max_q_value = max(_q_table[next_position][next_velocity])\n",
    "\n",
    "    \n",
    "    position, velocity = get_status(_observation)\n",
    "    q_value = _q_table[position][velocity][_action]\n",
    "\n",
    "   \n",
    "    _q_table[position][velocity][_action] = q_value + alpha * (_reward + gamma * next_max_q_value - q_value)\n",
    "\n",
    "    return _q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(_env, _q_table, _observation, _episode):\n",
    "    epsilon = 0.02\n",
    "    if np.random.uniform(0, 1) > epsilon:\n",
    "        position, velocity = get_status(observation)\n",
    "        _action = np.argmax(_q_table[position][velocity])\n",
    "    else:\n",
    "        _action = np.random.choice([0, 1, 2])\n",
    "    return _action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    env = gym.make('MountainCar-v0')\n",
    "\n",
    "\n",
    "    q_table = np.zeros((40, 40, 3))\n",
    "\n",
    "    observation = env.reset()\n",
    "    rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, total_reward: -144.0\n",
      "episode: 100, total_reward: -189.0\n",
      "episode: 200, total_reward: -200.0\n",
      "episode: 300, total_reward: -200.0\n",
      "episode: 400, total_reward: -200.0\n",
      "episode: 500, total_reward: -144.0\n",
      "episode: 600, total_reward: -124.0\n",
      "episode: 700, total_reward: -125.0\n",
      "episode: 800, total_reward: -125.0\n",
      "episode: 900, total_reward: -150.0\n",
      "episode: 1000, total_reward: -128.0\n",
      "episode: 1100, total_reward: -145.0\n",
      "episode: 1200, total_reward: -120.0\n",
      "episode: 1300, total_reward: -145.0\n",
      "episode: 1400, total_reward: -148.0\n",
      "episode: 1500, total_reward: -133.0\n",
      "episode: 1600, total_reward: -149.0\n",
      "episode: 1700, total_reward: -144.0\n",
      "episode: 1800, total_reward: -151.0\n",
      "episode: 1900, total_reward: -154.0\n",
      "episode: 2000, total_reward: -116.0\n",
      "episode: 2100, total_reward: -193.0\n",
      "episode: 2200, total_reward: -119.0\n",
      "episode: 2300, total_reward: -152.0\n",
      "episode: 2400, total_reward: -159.0\n",
      "episode: 2500, total_reward: -149.0\n",
      "episode: 2600, total_reward: -113.0\n",
      "episode: 2700, total_reward: -150.0\n",
      "episode: 2800, total_reward: -147.0\n",
      "episode: 2900, total_reward: -150.0\n",
      "episode: 3000, total_reward: -158.0\n",
      "episode: 3100, total_reward: -152.0\n",
      "episode: 3200, total_reward: -147.0\n",
      "episode: 3300, total_reward: -144.0\n",
      "episode: 3400, total_reward: -147.0\n",
      "episode: 3500, total_reward: -188.0\n",
      "episode: 3600, total_reward: -166.0\n",
      "episode: 3700, total_reward: -135.0\n",
      "episode: 3800, total_reward: -148.0\n",
      "episode: 3900, total_reward: -155.0\n",
      "episode: 4000, total_reward: -120.0\n",
      "episode: 4100, total_reward: -146.0\n",
      "episode: 4200, total_reward: -144.0\n",
      "episode: 4300, total_reward: -164.0\n",
      "episode: 4400, total_reward: -132.0\n",
      "episode: 4500, total_reward: -145.0\n",
      "episode: 4600, total_reward: -156.0\n",
      "episode: 4700, total_reward: -139.0\n",
      "episode: 4800, total_reward: -119.0\n",
      "episode: 4900, total_reward: -156.0\n",
      "episode: 5000, total_reward: -200.0\n",
      "episode: 5100, total_reward: -131.0\n",
      "episode: 5200, total_reward: -197.0\n",
      "episode: 5300, total_reward: -154.0\n",
      "episode: 5400, total_reward: -175.0\n",
      "episode: 5500, total_reward: -147.0\n",
      "episode: 5600, total_reward: -200.0\n",
      "episode: 5700, total_reward: -191.0\n",
      "episode: 5800, total_reward: -143.0\n",
      "episode: 5900, total_reward: -138.0\n",
      "episode: 6000, total_reward: -150.0\n",
      "episode: 6100, total_reward: -136.0\n",
      "episode: 6200, total_reward: -181.0\n",
      "episode: 6300, total_reward: -186.0\n",
      "episode: 6400, total_reward: -147.0\n",
      "episode: 6500, total_reward: -148.0\n",
      "episode: 6600, total_reward: -191.0\n",
      "episode: 6700, total_reward: -186.0\n",
      "episode: 6800, total_reward: -138.0\n",
      "episode: 6900, total_reward: -142.0\n",
      "episode: 7000, total_reward: -181.0\n",
      "episode: 7100, total_reward: -156.0\n",
      "episode: 7200, total_reward: -138.0\n",
      "episode: 7300, total_reward: -146.0\n",
      "episode: 7400, total_reward: -119.0\n",
      "episode: 7500, total_reward: -154.0\n",
      "episode: 7600, total_reward: -120.0\n",
      "episode: 7700, total_reward: -116.0\n",
      "episode: 7800, total_reward: -152.0\n",
      "episode: 7900, total_reward: -159.0\n",
      "episode: 8000, total_reward: -150.0\n",
      "episode: 8100, total_reward: -142.0\n",
      "episode: 8200, total_reward: -165.0\n",
      "episode: 8300, total_reward: -147.0\n",
      "episode: 8400, total_reward: -154.0\n",
      "episode: 8500, total_reward: -150.0\n",
      "episode: 8600, total_reward: -143.0\n",
      "episode: 8700, total_reward: -184.0\n",
      "episode: 8800, total_reward: -120.0\n",
      "episode: 8900, total_reward: -112.0\n",
      "episode: 9000, total_reward: -143.0\n",
      "episode: 9100, total_reward: -112.0\n",
      "episode: 9200, total_reward: -118.0\n",
      "episode: 9300, total_reward: -139.0\n",
      "episode: 9400, total_reward: -137.0\n",
      "episode: 9500, total_reward: -113.0\n",
      "episode: 9600, total_reward: -140.0\n",
      "episode: 9700, total_reward: -136.0\n",
      "episode: 9800, total_reward: -136.0\n",
      "episode: 9900, total_reward: -147.0\n"
     ]
    }
   ],
   "source": [
    " for episode in range(10000):\n",
    "\n",
    "        total_reward = 0\n",
    "        observation = env.reset()\n",
    "          \n",
    "\n",
    "        for _ in range(200):\n",
    "     \n",
    "            action = get_action(env, q_table, observation, episode)\n",
    "\n",
    "            next_observation, reward, done, _ = env.step(action)\n",
    "\n",
    "            q_table = update_q_table(q_table, action, observation, next_observation, reward, episode)\n",
    "            total_reward += reward\n",
    "\n",
    "            observation = next_observation\n",
    "\n",
    "            if done:\n",
    "                if episode%100 == 0:\n",
    "                    print('episode: {}, total_reward: {}'.format(episode, total_reward))\n",
    "                rewards.append(total_reward)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "done = False\n",
    "reward = 0\n",
    "while not done:\n",
    "    env.render()\n",
    "     \n",
    "    action = get_action(env, q_table, observation, episode)\n",
    "\n",
    "    next_observation, reward, done, _ = env.step(action)\n",
    "\n",
    "    q_table = update_q_table(q_table, action, observation, next_observation, reward, episode)\n",
    "    total_reward += reward\n",
    "\n",
    "    observation = next_observation\n",
    "    \n",
    "    time.sleep(0.02)\n",
    "env.close()\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
